{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca5a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas openai datasets python-dotenv langchain_community \n",
    "# %pip install protobuf==3.20.0\n",
    "# %pip install langchain-core\n",
    "\n",
    "# %pip install langchain-openai\n",
    "# %pip install --upgrade langchain\n",
    "# %pip install selenium\n",
    "# %pip install unstructured\n",
    "# %pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b41f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting chromadb\n",
      "  Using cached chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.27.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (2.6.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (0.15.1)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m746.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (5.10.2)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.19.0-cp310-cp310-macosx_11_0_universal2.whl (16.8 MB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-3.6.0-py2.py3-none-any.whl (50 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (3.10.7)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-4.1.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (6.0)\n",
      "Collecting tqdm>=4.65.0\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (1.23.5)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-4.2.0-cp39-abi3-macosx_10_12_universal2.whl (472 kB)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (0.112.0)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting chroma-hnswlib==0.7.6\n",
      "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-macosx_11_0_arm64.whl (183 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from chromadb) (1.60.1)\n",
      "Collecting overrides>=7.3.1\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Collecting pyproject_hooks\n",
      "  Using cached pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: sniffio in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: anyio in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.6.2)\n",
      "Requirement already satisfied: idna in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.20)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: protobuf in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.0)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: sympy in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: flatbuffers in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1.21)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.27.0\n",
      "  Using cached opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opentelemetry-instrumentation==0.48b0\n",
      "  Using cached opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0\n",
      "  Using cached opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.1.0)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.16.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.0)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.3)\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-13.0.1-cp310-cp310-macosx_11_0_arm64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-macosx_11_0_arm64.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.4/367.4 kB\u001b[0m \u001b[31m799.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httptools>=0.5.0\n",
      "  Using cached httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl (149 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "  Downloading uvloop-0.20.0-cp310-cp310-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.0)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Requirement already satisfied: filelock in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53726 sha256=040d0c10f0d0f6c99ff3d0a312e11eefaf1e814c6b88f7f1ad10ea8ed4ad310e\n",
      "  Stored in directory: /Users/mattsalomon/Library/Caches/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, tqdm, tomli, shellingham, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, asgiref, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, typer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: tqdm\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: protobuf\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.4 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.4 which is incompatible.\n",
      "langchain-openai 0.0.6 requires langchain-core<0.2,>=0.1.16, but you have langchain-core 0.2.37 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.2.0 build-1.2.1 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 deprecated-1.2.14 httptools-0.6.1 humanfriendly-10.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.19.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.6.0 protobuf-4.25.4 pypika-0.48.9 pyproject_hooks-1.1.0 shellingham-1.5.4 tomli-2.0.1 tqdm-4.66.5 typer-0.12.5 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-core (/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install webdriver_manager\n",
    "#%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04a44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d6cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# text_splitter = SemanticChunker(embedding_function)\n",
    "# docs = text_splitter.create_documents('test sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2493b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "# # Sample document content (simulated example)\n",
    "# documents = [\n",
    "#     \"New York City comprises 5 boroughs sitting where the Hudson River meets the Atlantic Ocean. At its core is Manhattan, a densely populated borough that’s among the world’s major commercial, financial, and cultural centers.\",\n",
    "#     \"Snow leopards are large cats native to the mountain ranges of Central and South Asia. They are listed as Vulnerable on the IUCN Red List because of declining populations.\",\n",
    "#     \"The Galápagos Islands are a volcanic archipelago in the Pacific Ocean. It’s considered one of the world’s foremost destinations for wildlife-viewing.\",\n",
    "#     \"Penguins are flightless birds that live almost exclusively in the Southern Hemisphere, especially in Antarctica. They are highly adapted to life in the water.\"\n",
    "# ]\n",
    "\n",
    "# # Initialize a CharacterTextSplitter\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\",  # Split on newlines or any other separator\n",
    "#     chunk_size=100,  # Maximum size of each chunk\n",
    "#     chunk_overlap=20  # Overlap between chunks\n",
    "# )\n",
    "\n",
    "# # Split each document into smaller chunks\n",
    "# split_documents = []\n",
    "# for document in documents:\n",
    "#     split_chunks = text_splitter.split_text(document)\n",
    "#     split_documents.extend(split_chunks)\n",
    "\n",
    "# # Display the split documents\n",
    "# for idx, chunk in enumerate(split_documents):\n",
    "#     print(f\"Chunk {idx + 1}:\\n{chunk}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213224c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattsalomon/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Embedding: [-0.03306525 -0.04929627  0.00117881 -0.05240887 -0.03758711  0.02581971\n",
      " -0.03928511  0.05620158  0.09028886 -0.05235003]...\n",
      "Document 2 Embedding: [-0.06268118 -0.07143187 -0.01995398 -0.04276575  0.00905974  0.07724482\n",
      " -0.01931657  0.05321806  0.1186787  -0.06624138]...\n",
      "Document 3 Embedding: [-0.02030906 -0.12023307 -0.06489139 -0.03242574 -0.01917392 -0.00842921\n",
      " -0.06926634  0.04791469  0.06779966 -0.05391078]...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Example long text\n",
    "long_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "It enables you to connect a variety of language models to your applications. \n",
    "You can chain together different language models or other components, such as data sources, to create powerful, complex applications.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Split based on newline characters\n",
    "    chunk_size=100,  # Max number of characters in each chunk\n",
    "    chunk_overlap=0  # No overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the text into smaller chunks\n",
    "text_chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "# Create documents from the text chunks\n",
    "docs = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "# Load a pre-trained embedding model from sentence_transformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed each document's content\n",
    "embeddings = [model.encode(doc.page_content) for doc in docs]\n",
    "\n",
    "# Print the embeddings for each document\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Document {i + 1} Embedding: {embedding[:10]}...\")  # Print first 10 dimensions for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87f81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c7a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# # Example long text\n",
    "# long_text = \"\"\"\n",
    "# LangChain is a framework for developing applications powered by language models. \n",
    "# It enables you to connect a variety of language models to your applications. \n",
    "# You can chain together different language models or other components, such as data sources, to create powerful, complex applications.\n",
    "# \"\"\"\n",
    "\n",
    "# # Initialize the text splitter\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\",  # Split based on newline characters\n",
    "#     chunk_size=100,  # Max number of characters in each chunk\n",
    "#     chunk_overlap=0  # No overlap between chunks\n",
    "# )\n",
    "\n",
    "# # Split the text into smaller chunks\n",
    "# text_chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "# # Create documents from the text chunks\n",
    "# docs = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "# # Print the created documents\n",
    "# for doc in docs:\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73c2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/New_York_City\",\n",
    "    \"https://en.wikipedia.org/wiki/Snow_leopard\",\n",
    "    \"https://www.britannica.com/place/Galapagos-Islands\",\n",
    "    \"https://www.birdlife.org/birds/penguins/#:~:text=The%20threats%20are%20numerous%2C%20including,is%20melting%20before%20their%20eyes.\"\n",
    "]\n",
    "\n",
    "# collect data using selenium url loader\n",
    "# loader = SeleniumURLLoader(urls=urls)\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f8bef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def load_content_with_selenium(url):\n",
    "    # Setup Chrome options for faster performance\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run headless for speed\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU for headless\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Create a WebDriver with a timeout\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.set_page_load_timeout(10)  # Set a page load timeout of 10 seconds\n",
    "    driver.implicitly_wait(5)  # Set an implicit wait of 5 seconds\n",
    "    #driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.set_page_load_timeout(10)  # Set a timeout of 10 seconds\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Now pass the loaded page source to a loader or process as needed\n",
    "        page_source = driver.page_source\n",
    "        # Example: If you want to use the SeleniumURLLoader after loading:\n",
    "        loader = SeleniumURLLoader(urls=[url])\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/New_York_City\",\n",
    "    \"https://en.wikipedia.org/wiki/Snow_leopard\",\n",
    "   # \"https://www.britannica.com/place/Galapagos-Islands\",\n",
    "    \"https://www.birdlife.org/birds/penguins/#:~:text=The%20threats%20are%20numerous%2C%20including,is%20melting%20before%20their%20eyes.\"\n",
    "]\n",
    "\n",
    "# Load content for each URL\n",
    "documents = []\n",
    "for url in urls:\n",
    "    docs = load_content_with_selenium(url)\n",
    "    if docs:\n",
    "        documents.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e217072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57622138",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentList = []\n",
    "for doc in documents:\n",
    "    d = str(doc.page_content).replace(\"\\\\n\", \" \").replace(\"\\\\t\",\" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    documentList.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "359f0f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\npage_content\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(documentList))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create documents from the text chunks\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mchunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load a pre-trained embedding model from sentence_transformers\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(documentList))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create documents from the text chunks\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load a pre-trained embedding model from sentence_transformers\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/documents/base.py:270\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[0;34m(self, page_content, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pass page_content in as positional or named arg.\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# my-py is complaining that page_content is not defined on the base class.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Here, we're relying on pydantic base class to handle the validation.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# #text_splitter = SemanticChunker(embedding_function)\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the text splitter\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     #separator=\"\\n\",  # Split based on newline characters\n",
    "#     chunk_size=100,  # Max number of characters in each chunk\n",
    "#     chunk_overlap=0  # No overlap between chunks\n",
    "# )\n",
    "\n",
    "# # Split the text into smaller chunks\n",
    "# text_chunks = text_splitter.create_documents(''.join(documentList))\n",
    "\n",
    "# # Create documents from the text chunks\n",
    "# docs = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "# # Load a pre-trained embedding model from sentence_transformers\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Embed each document's content\n",
    "# embeddings = [model.encode(doc.page_content) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b804d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "all_texts = []\n",
    "\n",
    "embeddings_deployment = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embeddings_deployment,\n",
    "    # With the `text-embedding-3` class\n",
    "    # of models, you can specify the size\n",
    "    # of the embeddings you want returned.\n",
    "    # dimensions=1024\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "texts = text_splitter.split_text(''.join(documentList))\n",
    "\n",
    "# Add the chunks and metadata to the list\n",
    "all_texts.extend(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81a09111",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_texts(\n",
    "    all_texts, embeddings#, metadatas=metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47bdacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the text splitter\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\",  # Split based on newline characters\n",
    "#     chunk_size=100,  # Max number of characters in each chunk\n",
    "#     chunk_overlap=0  # No overlap between chunks\n",
    "# )\n",
    "\n",
    "# # Create documents using the text splitter\n",
    "# documents = text_splitter.create_documents([''.join(documentList)])\n",
    "\n",
    "# # Load a pre-trained embedding model from sentence_transformers\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Embed each document's content\n",
    "# for doc in documents:\n",
    "#     embedding = model.encode(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86ebf1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e86a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # storing embeddings in a folder\n",
    "# vector_store = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# # use this to load vector database\n",
    "# vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e5aecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Go through the context and answer given question strictly based on context. \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm = ChatOpenAI(temperature=0),\n",
    "        # retriever=vector_store.as_retriever(search_kwargs={'k': 3}),\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PromptTemplate.from_template(PROMPT_TEMPLATE)}\n",
    "    )\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(),\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=vector_store.as_retriever(search_kwargs={\"k\": 1}),\n",
    "                                       return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PromptTemplate.from_template(PROMPT_TEMPLATE)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd565ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Who discovered the Galapagos Islands and how?\",\n",
    "    \"What is Brooklyn–Battery Tunnel?\",\n",
    "    \"Are Penguins found in the Galapagos Islands?\",\n",
    "    \"How many languages are spoken in New York?\",\n",
    "    \"In which countries are snow leopards found?\",\n",
    "    \"What are the threats to penguin populations?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"How did Galapagos Islands get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "    \n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"The Galapagos Islands were discovered in 1535 by the bishop of Panama, Tomás de Berlanga, whose ship had drifted off course while en route to Peru. He named them Las Encantadas (“The Enchanted”), and in his writings he marveled at the thousands of large galápagos (tortoises) found there. Numerous Spanish voyagers stopped at the islands from the 16th century, and the Galapagos also came to be used by pirates and by whale and seal hunters. \",\n",
    "    \"The Brooklyn-Battery Tunnel (officially known as the Hugh L. Carey Tunnel) is the longest continuous underwater vehicular tunnel in North America and runs underneath Battery Park, connecting the Financial District in Lower Manhattan to Red Hook in Brooklyn.[586]\",\n",
    "    \"Penguins live on the galapagos islands side by side with tropical animals.\",\n",
    "    \"As many as 800 languages are spoken in New York.\",\n",
    "    \"Siberia, Tajikistan, Kyrgyzstan, Uzbekistan, Kazakhstan, Afghanistan, Pakistan, India, Nepal, Bhutan, Mongolia, and Tibet.\",\n",
    "    \"The threats are numerous, including habitat loss, pollution, disease, and reduced food availability due to commercial fishing. Climate change is of particular concern for many species of penguin, as the sea ice that they depend on to find food or build nests is melting before their eyes.\",\n",
    "    \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",\n",
    "    \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",\n",
    "    \"Tomás de Berlanga, who discovered the islands, named them Las Encantadas (“The Enchanted”), and in his writings he marveled at the thousands of large galápagos (tortoises) found there. Numerous Spanish voyagers stopped at the islands from the 16th century, and the Galapagos also came to be used by pirates and by whale and seal hunters.\",\n",
    "    \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4d67c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "contexts = []\n",
    "for query in queries:\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "   \n",
    "    results.append(result['result'])\n",
    "    sources = result[\"source_documents\"]\n",
    "    contents = []\n",
    "    for i in range(len(sources)):\n",
    "        contents.append(sources[i].page_content)\n",
    "    contexts.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4bd7032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'result', 'source_documents'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cb862aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7863d4ba554d4d37997d2fca18fe1ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"question\": queries,\n",
    "    \"answer\": results,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(d)\n",
    "score = evaluate(dataset,metrics=[faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness, harmfulness])\n",
    "score_df = score.to_pandas()\n",
    "score_df.to_csv(\"EvaluationScores.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72c23a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faithfulness             1.000000\n",
       "answer_relevancy         0.760905\n",
       "context_precision        0.800000\n",
       "context_recall           0.683333\n",
       "context_entity_recall    0.326667\n",
       "answer_similarity        0.919008\n",
       "answer_correctness       0.756288\n",
       "harmfulness              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df[['faithfulness','answer_relevancy', 'context_precision', 'context_recall',\n",
    "       'context_entity_recall', 'answer_similarity', 'answer_correctness',\n",
    "       'harmfulness']].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "107fb0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who discovered the Galapagos Islands and how?</td>\n",
       "      <td>The context does not mention anything about th...</td>\n",
       "      <td>[of the Hudson River, which he named Río de Sa...</td>\n",
       "      <td>The Galapagos Islands were discovered in 1535 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Brooklyn–Battery Tunnel?</td>\n",
       "      <td>The Brooklyn-Battery Tunnel is a vehicular tun...</td>\n",
       "      <td>[Park Service. Archived from the original on J...</td>\n",
       "      <td>The Brooklyn-Battery Tunnel (officially known ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.951870</td>\n",
       "      <td>0.737967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are Penguins found in the Galapagos Islands?</td>\n",
       "      <td>Yes, penguins can be spotted on the volcanic i...</td>\n",
       "      <td>[Learn more about each species of penguin and ...</td>\n",
       "      <td>Penguins live on the galapagos islands side by...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972815</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910159</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many languages are spoken in New York?</td>\n",
       "      <td>As many as 800 languages are spoken in New Yor...</td>\n",
       "      <td>[2017. \"The immigrant share of the population ...</td>\n",
       "      <td>As many as 800 languages are spoken in New York.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which countries are snow leopards found?</td>\n",
       "      <td>Snow leopards are found in southern Siberia, T...</td>\n",
       "      <td>[Baikal through southern Siberia, in the Kunlu...</td>\n",
       "      <td>Siberia, Tajikistan, Kyrgyzstan, Uzbekistan, K...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.871289</td>\n",
       "      <td>0.967822</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the threats to penguin populations?</td>\n",
       "      <td>The threats to penguin populations include hab...</td>\n",
       "      <td>[ice or huddle together for warmth will melt t...</td>\n",
       "      <td>The threats are numerous, including habitat lo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.958360</td>\n",
       "      <td>0.864598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the economic significance of New York ...</td>\n",
       "      <td>The economic significance of New York City inc...</td>\n",
       "      <td>[metropolitan economy, with a gross metropolit...</td>\n",
       "      <td>New York City's economic significance is vast,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.954169</td>\n",
       "      <td>0.827828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How did New York City get its name?</td>\n",
       "      <td>New York City was temporarily renamed New York...</td>\n",
       "      <td>[city in 1653. The city came under English con...</td>\n",
       "      <td>New York City got its name when it came under ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.953170</td>\n",
       "      <td>0.613292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How did Galapagos Islands get its name?</td>\n",
       "      <td>The context provided does not mention how the ...</td>\n",
       "      <td>[lynx), is where the Latin name uncia and the ...</td>\n",
       "      <td>Tomás de Berlanga, who discovered the islands,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838250</td>\n",
       "      <td>0.209549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the significance of the Statue of Libe...</td>\n",
       "      <td>The Statue of Liberty was a reassuring sign fo...</td>\n",
       "      <td>[States ever since.\"  ^ The Immigrant's Statue...</td>\n",
       "      <td>The Statue of Liberty in New York City holds g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.936578</td>\n",
       "      <td>0.609145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0      Who discovered the Galapagos Islands and how?   \n",
       "1                   What is Brooklyn–Battery Tunnel?   \n",
       "2       Are Penguins found in the Galapagos Islands?   \n",
       "3         How many languages are spoken in New York?   \n",
       "4        In which countries are snow leopards found?   \n",
       "5       What are the threats to penguin populations?   \n",
       "6  What is the economic significance of New York ...   \n",
       "7                How did New York City get its name?   \n",
       "8            How did Galapagos Islands get its name?   \n",
       "9  What is the significance of the Statue of Libe...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The context does not mention anything about th...   \n",
       "1  The Brooklyn-Battery Tunnel is a vehicular tun...   \n",
       "2  Yes, penguins can be spotted on the volcanic i...   \n",
       "3  As many as 800 languages are spoken in New Yor...   \n",
       "4  Snow leopards are found in southern Siberia, T...   \n",
       "5  The threats to penguin populations include hab...   \n",
       "6  The economic significance of New York City inc...   \n",
       "7  New York City was temporarily renamed New York...   \n",
       "8  The context provided does not mention how the ...   \n",
       "9  The Statue of Liberty was a reassuring sign fo...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [of the Hudson River, which he named Río de Sa...   \n",
       "1  [Park Service. Archived from the original on J...   \n",
       "2  [Learn more about each species of penguin and ...   \n",
       "3  [2017. \"The immigrant share of the population ...   \n",
       "4  [Baikal through southern Siberia, in the Kunlu...   \n",
       "5  [ice or huddle together for warmth will melt t...   \n",
       "6  [metropolitan economy, with a gross metropolit...   \n",
       "7  [city in 1653. The city came under English con...   \n",
       "8  [lynx), is where the Latin name uncia and the ...   \n",
       "9  [States ever since.\"  ^ The Immigrant's Statue...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  The Galapagos Islands were discovered in 1535 ...           1.0   \n",
       "1  The Brooklyn-Battery Tunnel (officially known ...           1.0   \n",
       "2  Penguins live on the galapagos islands side by...           1.0   \n",
       "3   As many as 800 languages are spoken in New York.           1.0   \n",
       "4  Siberia, Tajikistan, Kyrgyzstan, Uzbekistan, K...           1.0   \n",
       "5  The threats are numerous, including habitat lo...           1.0   \n",
       "6  New York City's economic significance is vast,...           1.0   \n",
       "7  New York City got its name when it came under ...           1.0   \n",
       "8  Tomás de Berlanga, who discovered the islands,...           1.0   \n",
       "9  The Statue of Liberty in New York City holds g...           1.0   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  context_entity_recall  \\\n",
       "0          0.000000                0.0        0.000000               0.000000   \n",
       "1          0.991293                1.0        0.500000               0.500000   \n",
       "2          0.972815                1.0        1.000000               0.000000   \n",
       "3          0.992015                1.0        1.000000               1.000000   \n",
       "4          0.968004                1.0        1.000000               0.666667   \n",
       "5          0.999998                1.0        1.000000               0.250000   \n",
       "6          0.905527                1.0        1.000000               0.100000   \n",
       "7          0.866140                1.0        1.000000               0.500000   \n",
       "8          0.000000                0.0        0.000000               0.000000   \n",
       "9          0.913256                1.0        0.333333               0.250000   \n",
       "\n",
       "   answer_similarity  answer_correctness  harmfulness  \n",
       "0           0.820816                 NaN            0  \n",
       "1           0.951870            0.737967            0  \n",
       "2           0.910159            0.977540            0  \n",
       "3           0.995414            0.998854            0  \n",
       "4           0.871289            0.967822            0  \n",
       "5           0.958360            0.864598            0  \n",
       "6           0.954169            0.827828            0  \n",
       "7           0.953170            0.613292            0  \n",
       "8           0.838250            0.209549            0  \n",
       "9           0.936578            0.609145            0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf35ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
